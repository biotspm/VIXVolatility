%!TEX root = ../Main.tex

\section{Selected volatility concepts and models of volatility measurement}
This section presents first some stylized facts of financial data, and gives an introduction to the different ways to estimate volatility. By pointing out the advantages and disadvantages of the concepts and models and their fit to the stylized facts, the HAR-RV approach shall be motivated. 


\subsection{The Return Process and Stylized Facts of Financial Data}
As mentioned in the introduction, the challenge when measuring volatility is, that stock return volatility is not directly observable \parencite{tsay2005}. This problem evolves from the fact that we can only observe one realization of the underlying data generating process, and even though stocks are traded and thus have market prices which could be used for volatility measurement, there is no continuous data available and even for high-frequency data and extremely liquid markets microstructure effects and noise prevent getting close to a continuous sample path. It is thus only possible to estimate avearages of discrete volatility for a given period of time. \parencite{andersen2001}.\\
There are however several approaches that should be introduced here. To start with, the definition of the simple gross return is
\begin{align}\label{eq:return}
1+ R_{t} = \frac{P_{t}}{P_{t-1}} 
\end{align}
In the continuous-time setting, continuously compounded returns are used, which are given by
\begin{align}\label{eq:log-return}
r_{t} = ln(1 + R_{t}) = ln (\frac{P_{t}}{P_{t}} + \frac{P_{t-1} - P_{t}}{P_{t}}) = 
ln \frac{P_{t}}{P_{t-1}} = p_{t} - p_{t-1} \ 
with\  p_{t} = ln(P_{t})
\end{align}
When observed over time, this asset returns show some distributional properties, often referred to as stylized facts of asset returns. Observed by many authors, only a few shall be mentioned here. \citeauthor{corsi2009} for example mentions particularly the very strong persistence of autocorrelation of the square and absolute returns, which regularly poses challenges to econometric models. Moreover return probability density functions are often leptocurtic with fat tails. As the time scale increases, the return distribution slowly converge to the normal distribution, but before convergence, the return distribution has different shapes depending on the time scale. Financial data also show evidence of scaling as described firstly by Mandelbrot, which is connected to the idea that patterns appear in different times, or that the distribution for returns has similar functional forms for various choices of the time interval. \\
Moreover \citeauthor{andersen2001} mentions that first, even though raw returns have a leptocurtic distribution, the returns standardized by realized volatility are approximately Gaussian. Second, the distribution of realized volatility of returns itself is right skewed, the one of the logarithms of realized volatility however are also approximately Gaussian. Third, the long-run dynamics of realized logarithmic volatilities are well approximated by a fractionally-integrated long-memory process. Other authors who mention stylized facts: \parencite{jiang2005}.
Moreover, even though volatility is not directly observable, it has some properties that are commonly seen. Here the characteristics as presented by \citeauthor{tsay2005} should be introduced. These are firstly volatility clusters, loosely speaking meaning that high volatility tends to be followed by high volatility, and the same is true for low volatility periods. Moreover, volatility jumps are rare. Third, volatility varies within a fixed range and does not diverge to infinity, which statistically means that volatility is often (weakly) stationary. Fourth, there is a leverage effect in volatility, meaning that it reads differently to a big increase in price than to a drop. 

\subsection{Concepts and Models using Historic Volatility}
\subsubsection{Volatility Concept and Non-parametric ex-post Volatility Measurement - Realized Volatility}
By definition ``volatility seeks to capture the strength of the (unexpected) return variation over a given period of time'' \parencite[p.7]{andersen2001}. However, there are multiple concepts and definitions of asset volatility. According to \citeauthor{andersen2001} the concepts can be grouped in (i) the \emph{notional volatility} corresponding to the ex-post sample-path return variability over a fixed time interval, (ii) the ex-ante \emph{expected volatility} over a fixed time interval or the (iii) the \emph{instantaneous volatility} corresponding to the strength of the volatility process at a point in time.
The aim of the HAR-RV model introduced later is to model volatility realized ex-post and then maybe use this approach to build a forecast model. Thus to define the left-hand-side variable the concept of realized volatility has to be introduced.
It can be shown, that under some assumptions, realized volatility as the square root of the sum of squared high frequency returns, can be used to approximate the quadratic variation process which is the variation in a continuous time setting. This approach mainly building on the work of \citeauthor{andersen2001} und [noch jemanden finden] shall only be briefly introduced here. \\
To begin with, it should be assumed that we have a continuous-time no-arbitrage setting. As return volatility aims to capture the strength of the unexpected return variation, one needs to define the component of a price change as opposed to an expected price movement. This requires the decomposition of the return process in an expected and an innovation component. \citeauthor{andersen2001} show, that under certain assumptions the instantaneous return process can be decomposed into an expected return component, and a martingale innovation (in the discrete time setting this decomposition is more complex, for this paper it shall only be relevant that the martingale part is still the dominant contribution to the return variation over short intervals). As mentioned in the previous paragraph, volatility measures often focus on representing the average volatility over a discrete time, as a continuous record of price data is not available, and even for liquid markets it is distorted by microstructure effects. \textcite{andersen2001} show that in order to measure the average variance\footnote{actually \textcite{andersen2001} term this the \emph{realized volatility}, but this paper will use the terminology of \textcite{corsi2009}, who uses the term realized volatility for the square-root of the integrated variance.}, one can refer to the quadratic variation process of this martingale component, as the quadratic variation process represents the (cumulative) realized sample path variability of the martingale over any fixed time interval. To be precise, they define \emph{notional variance} as the increment to the quadratic variation for the return series, measured ex-post. \\
Assuming that the mean of the return process is zero, taking the expected value of the notional variance and extending this concept slightly, one gets the \emph{realized variance}, defined over the $[t-h,t], 0 < h \leq t \leq T$ time interval as
\begin{align}\label{eq:RV-andersen}
v^2(t,h;n) = \sum_{i=1}^{n} r(t-h+(i/n) \times h,h/n)^2.
\end{align}
\citeauthor{andersen2001} show not only that the realized variance is an \emph{unbiased} estimator of ex-ante expected variance, or at least approximately unbiased when relaxing the zero mean assumption and taking a high sample frequency (their proposition 4). Moreover, \citeauthor{andersen2001} show that the realized variance is a \emph{consistent} nonparametric measure of the notional variance for increasingly finely sampled returns over any fixed length interval (their proposition 5).\\
So in summary, the increment to the quadratic return variation and thus past variance can be consistently and well approximated through the accumulation of high-frequency squared returns. Taking the square root of the realized variance, one gets the \emph{realized volatility}.\\ 


\subsubsection{Volatility Model - HAR-RV Model}
Having introduced the concept of notional volatility and it's approximation by realized volatility, we now turn to volatility modelling/measurement. To measure volatility, one can separate between parametric and non-parametric methods. Whereas parametric methods try to measure the expected volatility making different assumptions about both the functional form and the variables in the information set available, non-parametric methods try to quantify notional volatility directly. The realized volatility is an example for a non-parametric methods. However, to estimate volatility ex-ante (and to potentially forecast later), this paper will refer to one type of the parametric methods based on the paper of \textcite{corsi2009} termed the HAR-RV model.\\
In the last paragraph, volatility was measured as the average over discrete time. For the HAR-RV model this concept needs to be transferred to an instantaneous volatility measurement in continuous time. Moving to continuous time, the corresponding logarithmic price process is then given by \textcite{corsi2009} as the following stochastic difference equation,
\begin{align}\label{eq:return-process-corsi}
dp(t) = \mu(t)dt + \sigma(t)dW(t), \ 0 \leq t \leq T
\end{align}
where $p(t)$ is the logarithm of the instantaneous price, $\mu$ is a finite variation stochastic process, $W(s)$ standard Brownian motion and $\sigma$ a stochastic process independent of $W(s)$. For the volatility the difference is, that now the notional variance equals the \emph{integrated variance (IV)}, which is the integral of the stochastic process. For a period of one day, this is represented by
\begin{align}
IV_{t}^{(d)} =  \int_{t-1d}^{t} \sigma^{2}(w)dw.
\end{align}
\textcite{corsi2009} then take the square-root of this integrated variance and term it \emph{integrated volatility}. \textcite{andersen2001} show that also in this continuous-time setting the integrated variance equals the notional variance, and thus can also be approximated with the sum of squared returns. This leads to the following notion of realized volatility over the one day interval
\begin{align}
RV_{t}^{(d)} = \sqrt{\sum_{j=0}^{M-1} r^{2}_t-j \times \Delta}
\end{align}
with $\Delta = 1d/M$ being the sampling frequency and $r^{2}_t-j \times \Delta$ defined as the continuously compounded $\Delta$-frequency returns. \\
This notion of volatility is combined with the \emph{Heterogeneous Market Hypothesis} by \textcite{mueller1993}. This approach grounds on the fractal approach of Mandelbrot's 1983 paper, where (time series) objects are analyzed on different time scales and the obtained results are compared. The starting point of the argument is, that conventional time series analysis focusing on regularly spaced observations does not capture the real nature of the raw data, as the usual time choice for recording observations (e.g. a day) is arbitrary. Thus for example a process explaining monthly price changes could not be applied to daily data and vice versa. This fractal approach led to the heterogeneous market hypothesis. This hypothesis states that the market gives rise to heterogeneous trading behaviours as different market participants or components have different time horizons for their trading goals and for their consideration of past events. The time span has on the one side the high-frequency dealers such as market makers, in the middle some medium term dealers and on the other side the low-frequency dealers, such as central banks or commercial organizations. As the market is driven by these components, it can be described as ``fractal''. \textcite{mueller1993} base this theory of different time horizons on multiple observations: Firstly, the decline of the return autocorrelation function is not exponential, as suggested for example by lower-order GARCH or ARCH models, but rather hyperbolic. Assuming, that each of the distinct components has an exponential decline with different time horizons, this in sum comes close to a hyperbolic decline. Secondly, if market participants were homogeneous, volatility should be negatively correlated with market activity, as the price should converge to the ``real value''. However, they are positively correlated, which might be explained by the fact that actors react and execute in different market situations \parencite{mueller1993}. Other than to time scale, the heterogeneous market hypothesis can be applied to geographical location, degrees of risk aversion, institutional constraints or transactions costs. However, as in \textcite{corsi2009} for this paper the time aspect should be relevant. \textcite{corsi2009} refers to studies that measure volatility over different time horizons and observe that there is an asymmetric behaviour of volatility: ``The volatility over longer time intervals has a stronger influence on volatiilty over shorter time intervals than converseley'' \parencite[p.178]{corsi2009}. Thus a cascade pattern from low to high frequencies emerges. To formalize the model the latent partial volatility $\sigma_{t}^{(d)}$ is defined as the volatility generated by a certain market component (here daily). To account for short-term, medium-term and long-term traders, the time horizons of one day $(d)$, one week $(w)$ and one month $(m)$ are considered. Each of this volatility components corresponds to a market component, that forms the expectation for the volatility of the next period based on both the observation of the current realized volatility according to the own time frame, and on the expectation of the one horizon longer volatility. This formalizes in a cascade model, written as
\begin{align}\label{eq:cascade-model}
\sigma_{t+1d}^{(d)} = c + \beta^{d} RV_{t}^{(d)} + \beta^{(w)} RV_{t}^{(w)} + \beta^{(m)} RV_{t}^{(m)} + \tilde{w}_{t+1d}^{(d)},
\end{align}
with $\tilde{w}_{t+1d}^{(d)}$ being the innovation term.
Transferring this to a time series model, the ex-post approximation of latent volatility with realized volatility is used, which can be written as
\begin{align}\label{eq:ex-post-latent-vola}
\sigma_{t+1d}^{(d)} = RV_{t+1d}^{(d)} + w_{1+1d}^{(d)}.
\end{align}
where the $w_{t}^{(d)}$ now subsumes both the latent volatility measurement and estimation errors.  Taking equations \ref{eq:cascade-model} and \ref{eq:ex-post-latent-vola} together, we obtain the time series representation from the cascade model,
\begin{align}\label{eq:time-series-model}
RV_{t+1d}^{(d)} = c + \beta^{d} RV_{t}^{(d)} + \beta^{(w)} RV_{t}^{(w)} + \beta^{(m)} RV_{t}^{(m)} + w_{t+1d} ,
\end{align}
where $w_{t+1d} = \tilde{w}_{1+d}^{(d)} - w_{1+d}^{(d)}$. 
Using simulated data, \textcite{corsi2009} shows, that the HAR-RV simulated returns and volatility reproduce the stylized facts mentioned above very well. The data has not only the excess of kurtosis , but also the tail cross-over, meaning that the fat tails get thinner as the aggregation level increases. Concerning the volatility memory, the simulated data is also able to reproduce the long memory of the empirical data. Moreover they use OLS and forecast to show [complete this section, saying that they also show that there is information content and forecasting ability].


\subsection{Implied volatility}
\subsubsection{The General Idea of Implied Volatility}
As mentioned, there are multiple ways to model and measure volatility. One approach is to use not only historic price data, but to augment the information set by incorporating options data and using their forward looking nature. This approach can be termed \emph{implied volatility}, and can consist for example of a parametric volatility model for returns, accompanied by an asset pricing model and the option price information\parencite{andersen2001}. The intuition behind this is, that option prices can be seen as reflecting market participants' expectations of the future movements of the underlying asset. Assuming that the marked is informationally efficient, as described by \textcite{fama1970}, and that the asset pricing model is correct, the implied volatility derived from this model should not only subsume all information contained in the models using historic volatility, but also be a more efficient forecast of future volatility \parencite{jiang2005}. \\
One popular example of implied volatility is the \emph{Black-and-Scholes implied volatility}, building on the Black-and-Scholes-Merton (\gls{BS}) asset pricing model as presented by \textcite{black1973}. One input for pricing options with the \gls{BS} model is the volatility of the underlying asset. Having the derivative prices available on the market, it is possible to extract a value for the expected volatility, by inverting the theoretical asset pricing model. It is however important to note, that all of these procedures depend on the assumptions that are made in the asset pricing model \parencite{andersen2001}. \\
Many studies examined the information content of implied volatility and it's forecasting ability for future volatility. Whereas earlier studies find that implied volatility contains little additional information content compared to historical volatility and has no forecasting ability, later studies corrected for several data and methodological problems and found evidence supporting the hypothesis that implied volatility has predictive power for future volatility \parencite{jiang2005}. An example for an earlier study who found that implied volatility does not incorporate the information from historic volatility and constitute poor forecast of future volatility using S\%P 500 options is for example \textcite{canina1993}. The corrections more recent research made included for example using longer time series extending the regime shift around the 1987 crash (\textcite{ canina1993} for example use only a 4-year range of data from 1983 to 1987), adapt high-frequency asset returns to provide a more accurate view or use non-overlapping samples. Examples of these studies supporing the evidence of implied volatility subsuimg information are ... \parencite{jiang2005}. \\
However, even though the recent evidence is in support of \gls{BS} implied volatility, there are several disadvantages of using the implied volatility with an asset pricing model. One disadvantage is, that the \gls{BS} implied volatility relies mostly on information from at-the-money options, which are generally the most actively traded ones. This however fails to incorporate information contained in other options. Moreover and more importantly, testing for the \gls{BS} implied volatility always means testing market efficiency and the \gls{BS} model jointly, as both assumptions have to be true. As it is not possible in this setting to test for one of these aspects separately, this tests are subject to model misspecification errors \parencite{jiang2005}. This is why an implied volatility that does not rely on an asset pricing model was introduced. 


\subsubsection{VIX and Model-Free Implied Volatility}
The idea of implied volatility was introduced by \textcite{britten2000}, who characterize price processes that are consistent with current option prices, extending the approach of \textcite{derman1994}, \textcite{kani1994}, \textcite{dupire1994}, \textcite{dupire1997} and \textcite{rubinstein1994} on implied distributions. Taking a complete set of option prices as given and extracting as much information as possible out of the underyling price process as possible \parencite{britten20000}, their work does not use any specific option pricing model to derive implied volatility and is only based on no-arbitrage conditions \parencite{jiang2005}. 



\begin{itemize}\itemsep0pt
\item explain basic idea of model-free implied volatility
\item advantages of model-free implied volatility: solved joint hypothesis problem (direct test of market efficiency), can incorporate not only at-the-money options,..
\item the VIX as the model-free implied volatility estimate from the Cboe 
\end{itemize}

\textcolor{gray}{
Primilary described and derived by \citeauthor{britten2000}. Instead of being based on a specific option pricing model, it is derived entirely from no-arbitrage conditions. After that some papers did various corrections, such as \citeauthor{jiang2005} extended the model so that is not derived under diffusion assumptions and generalized it to processes including random jumps. Two advantage of the model-free option implied volatility, are firstly that it has no pricing assumption and thus constitutes a direct test of the option market's informational efficiency, and not a joined test of market efficiency and an assumed option pricing model. Secondly it incorporates information from options across different strike prices. }
