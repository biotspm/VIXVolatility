%!TEX root = ../Main.tex

\section{Selected Estimation and Modelling Procuedures for Volatility}\label{sec:2Models}
As mentioned in \ref{sec:1Intro}, volatility can not be directly observed and thus as to be estimated. This section briefly presents a concept for estimating realized volatility, which is important when it comes to modelling of this volatility. In the following, one example for such a model very well capturing the the stylized facts observed with financial data is presented. 

\subsection{Estimating and Modeling Volatility using Historic Volatility}\label{sec:22Historic}
%
\subsubsection{Estimating Realized Volatility}\label{sec:221RV}
By definition ``volatility seeks to capture the strength of the (unexpected) return variation over a given period of time'' \parencite[p.7]{andersen2001}. As terminology is not consistent in previous research, return variance is simply the second moment distribution characteristic, and return volatility is the standard deviations, which is the square root of the return variance. \\
However, with volatility being a latent variable, there are multiple concepts for volatility. According to \citeauthor{andersen2001} they can be grouped in (i) the \emph{notional volatility} corresponding to the ex-post sample-path return variability over a fixed time interval, (ii) the ex-ante \emph{expected volatility} over a fixed time interval or the (iii) the \emph{instantaneous volatility} corresponding to the strength of the volatility process at a point in time. Fur the purpose of this paper, which aims at measuring the information content of model-free implied volatility, an estimate for the  notional ex-post sample-path return variability is needed. \\
Volatility measures usually represent the average volatility over a discrete time, as a continuous record of price data is not available, and even for very liquid markets, price data are distorted by micro-structure effects. It can however be shown, that under some assumptions the sum of squared high-frequency returns is a consistent estimator for the return variance. This results mainly builds on the work of \parencite{andersen2001} and will only be briefly introduced here.\\
%
To begin with, it should be assumed that we have a continuous-time no-arbitrage setting. As return volatility aims to capture the strength of the unexpected return variation as defined above, one needs to define the component of a price change as opposed to an expected price movement. This requires the decomposition of the return process in an expected and an innovation component. \textcite{andersen2001} show, that under certain assumptions the instantaneous return process can be decomposed into an expected return component, and a martingale innovation (in the discrete time setting this decomposition is more complex, for this paper it shall only be relevant that the martingale part is still the dominant contribution to the return variation over short intervals). Furthermore \textcite{andersen2001} show, that the cumulative sample path variability of this martingale component can be represented by a quadratic variation process. To be precise, they define ex-post measured \emph{notional variance} is the increment to the quadratic variation for the return series. Moreover they show, that this notional variance can be consistently estimated using high-frequency returns or a large sample of returns, with the \emph{realized variance}, defined as:
\begin{align}\label{eq:RV-andersen}
v^2(t,h;n) = \sum_{i=1}^{n} r(t-h+(i/n) \times h,h/n)^2
\end{align}
over any fixed $[t-h,t], 0 < h$ time interval (Their proposition 5).\\
What is more, they show that realized variance is an unbiased estimator of the expected variance.\\
So in summary, the increment to the quadratic return variation which is the past variance, can be consistently and well approximated through the accumulation of high-frequency squared returns. Taking the square root of this realized variance, one obtains the \emph{realized volatility}:
\\ 


\subsubsection{Modelling Volatility - HAR-RV Model}\label{sec:222HAR-RV}
When observed over time, asset returns and asset return variability show some distributional properties, often referred to as stylized facts. These include high excess kurtosis for daily return series, and clustering of return variability, meaning that periods of large volatility seem to be followed by high volatility, and periods of low volatility seem to be followed by low volatility \parencite{tsay2005}. Moreover the autocorrelations of the square and absolute returns show a very strong persistence over long time periods. If return distributions with regard to different time horizons are observed, they show not only the fat tails mentioned above, but also tail-crossover, meaning that the shape depends on the time scale. As the time scale increases, the return distribution slowly converge to the normal distribution, however very slowly. Moroever, financial data show evidence of scaling and multi-scaling\parencite{corsi2009}. 

Having introduced the concept of notional volatility and it's approximation by realized volatility, we now turn to volatility modelling/measurement. To measure volatility, one can separate between parametric and non-parametric methods. Whereas parametric methods try to measure the expected volatility making different assumptions about both the functional form and the variables in the information set available, non-parametric methods try to quantify notional volatility directly. The realized volatility is an example for a non-parametric methods. However, to estimate volatility ex-ante (and to potentially forecast later), this paper will refer to one type of the parametric methods based on the paper of \textcite{corsi2009} termed the HAR-RV model.\\
In the last paragraph, volatility was measured as the average over discrete time. For the HAR-RV model this concept needs to be transferred to an instantaneous volatility measurement in continuous time. Moving to continuous time, the corresponding logarithmic price process is then given by \textcite{corsi2009} as the following stochastic difference equation,
\begin{align}\label{eq:return-process-corsi}
dp(t) = \mu(t)dt + \sigma(t)dW(t), \ 0 \leq t \leq T
\end{align}
where $p(t)$ is the logarithm of the instantaneous price, $\mu$ is a finite variation stochastic process, $W(s)$ standard Brownian motion and $\sigma$ a stochastic process independent of $W(s)$. For the volatility the difference is, that now the notional variance equals the \emph{integrated variance (IV)}, which is the integral of the stochastic process. For a period of one day, this is represented by
\begin{align}
IV_{t}^{(d)} =  \int_{t-1d}^{t} \sigma^{2}(w)dw.
\end{align}
\textcite{corsi2009} then take the square-root of this integrated variance and term it \emph{integrated volatility}. \textcite{andersen2001} show that also in this continuous-time setting the integrated variance equals the notional variance, and thus can also be approximated with the sum of squared returns. This leads to the following notion of realized volatility over the one day interval
\begin{align}
RV_{t}^{(d)} = \sqrt{\sum_{j=0}^{M-1} r^{2}_t-j \times \Delta}
\end{align}
with $\Delta = 1d/M$ being the sampling frequency and $r^{2}_t-j \times \Delta$ defined as the continuously compounded $\Delta$-frequency returns. \\
This notion of volatility is combined with the \emph{Heterogeneous Market Hypothesis} by \textcite{mueller1993}. This approach grounds on the fractal approach of Mandelbrot's 1983 paper, where (time series) objects are analyzed on different time scales and the obtained results are compared. The starting point of the argument is, that conventional time series analysis focusing on regularly spaced observations does not capture the real nature of the raw data, as the usual time choice for recording observations (e.g. a day) is arbitrary. Thus for example a process explaining monthly price changes could not be applied to daily data and vice versa. This fractal approach led to the heterogeneous market hypothesis. This hypothesis states that the market gives rise to heterogeneous trading behaviours as different market participants or components have different time horizons for their trading goals and for their consideration of past events. The time span has on the one side the high-frequency dealers such as market makers, in the middle some medium term dealers and on the other side the low-frequency dealers, such as central banks or commercial organizations. As the market is driven by these components, it can be described as ``fractal''. \textcite{mueller1993} base this theory of different time horizons on multiple observations: Firstly, the decline of the return autocorrelation function is not exponential, as suggested for example by lower-order GARCH or ARCH models, but rather hyperbolic. Assuming, that each of the distinct components has an exponential decline with different time horizons, this in sum comes close to a hyperbolic decline. Secondly, if market participants were homogeneous, volatility should be negatively correlated with market activity, as the price should converge to the ``real value''. However, they are positively correlated, which might be explained by the fact that actors react and execute in different market situations \parencite{mueller1993}. Other than to time scale, the heterogeneous market hypothesis can be applied to geographical location, degrees of risk aversion, institutional constraints or transactions costs. However, as in \textcite{corsi2009} for this paper the time aspect should be relevant. \textcite{corsi2009} refers to studies that measure volatility over different time horizons and observe that there is an asymmetric behaviour of volatility: ``The volatility over longer time intervals has a stronger influence on volatiilty over shorter time intervals than converseley'' \parencite[p.178]{corsi2009}. Thus a cascade pattern from low to high frequencies emerges. To formalize the model the latent partial volatility $\sigma_{t}^{(d)}$ is defined as the volatility generated by a certain market component (here daily). To account for short-term, medium-term and long-term traders, the time horizons of one day $(d)$, one week $(w)$ and one month $(m)$ are considered. Each of this volatility components corresponds to a market component, that forms the expectation for the volatility of the next period based on both the observation of the current realized volatility according to the own time frame, and on the expectation of the one horizon longer volatility. This formalizes in a cascade model, written as
\begin{align}\label{eq:cascade-model}
\sigma_{t+1d}^{(d)} = c + \beta^{d} RV_{t}^{(d)} + \beta^{(w)} RV_{t}^{(w)} + \beta^{(m)} RV_{t}^{(m)} + \tilde{w}_{t+1d}^{(d)},
\end{align}
with $\tilde{w}_{t+1d}^{(d)}$ being the innovation term.
Transferring this to a time series model, the ex-post approximation of latent volatility with realized volatility is used, which can be written as
\begin{align}\label{eq:ex-post-latent-vola}
\sigma_{t+1d}^{(d)} = RV_{t+1d}^{(d)} + w_{1+1d}^{(d)}.
\end{align}
where the $w_{t}^{(d)}$ now subsumes both the latent volatility measurement and estimation errors.  Taking equations \ref{eq:cascade-model} and \ref{eq:ex-post-latent-vola} together, we obtain the time series representation from the cascade model,
\begin{align}\label{eq:time-series-model}
RV_{t+1d}^{(d)} = c + \beta^{d} RV_{t}^{(d)} + \beta^{(w)} RV_{t}^{(w)} + \beta^{(m)} RV_{t}^{(m)} + w_{t+1d} ,
\end{align}
where $w_{t+1d} = \tilde{w}_{1+d}^{(d)} - w_{1+d}^{(d)}$. 
Using simulated data, \textcite{corsi2009} shows, that the HAR-RV simulated returns and volatility reproduce the stylized facts mentioned above very well. The data has not only the excess of kurtosis , but also the tail cross-over, meaning that the fat tails get thinner as the aggregation level increases. Concerning the volatility memory, the simulated data is also able to reproduce the long memory of the empirical data. Moreover they use OLS and forecast to show [complete this section, saying that they also show that there is information content and forecasting ability].


\subsection{Estimating Volatility Using Option Price Data - Implied Volatility}
\subsubsection{The General Idea of Implied Volatility}
As mentioned, there are multiple ways to model and measure volatility. One approach is to use not only historic price data, but to augment the information set by incorporating options data and using their forward looking nature. This approach can be termed \emph{implied volatility}, and can consist for example of a parametric volatility model for returns, accompanied by an asset pricing model and the option price information\parencite{andersen2001}. The intuition behind this is, that option prices can be seen as reflecting market participants' expectations of the future movements of the underlying asset. Assuming that the marked is informationally efficient, as described by \textcite{fama1970}, and that the asset pricing model is correct, the implied volatility derived from this model should not only subsume all information contained in the models using historic volatility, but also be a more efficient forecast of future volatility \parencite{jiang2003}. \\
One popular example of implied volatility is the \emph{Black-and-Scholes implied volatility}, building on the Black-and-Scholes-Merton (\gls{BS}) asset pricing model as presented by \textcite{black1973}. One input for pricing options with the \gls{BS} model is the volatility of the underlying asset. Having the derivative prices available on the market, it is possible to extract a value for the expected volatility, by inverting the theoretical asset pricing model. It is however important to note, that all of these procedures depend on the assumptions that are made in the asset pricing model \parencite{andersen2001}. \\
%
Many studies examined the information content of \gls{BS} implied volatility and it's forecasting ability for future volatility. Earlier studies find that implied volatility contains little additional information content compared to historical volatility and has no forecasting ability \parencite{jiang2003}. \textcite{canina1993} for example found that implied volatility does not incorporate the information from historic volatility and constitute poor forecast of future volatility using S\&P 500 (\gls{SPX}) options. In contrast, other studies  support the evidence of implied volatility subsuming the information from historic volatility. These are for example \textcite{day1992}, \textcite{lamoureux1993} or \textcite{jorion1995}. \textcite{day1992} examine information content from S\&P 100 options between 1983 and 1989 and find that they have significant information content for realized volatility. With previous research leading ambiguous results, more recent research made several methodological and data corrections and found evidence supporting the hypothesis that implied volatility has predictive power for future volatility. These corrections included for example using longer time series extending the regime shift around the 1987 crash or adapting high-frequency asset returns to provide a more accurate view or use non-overlapping samples \parencite{jiang2003}. For example \textcite{christensen1998} find that subsumes the information content of past volatility and outperforms it in forecasting future volatility. They use a longer time series, take into account the 1987 regime shift and use non-overlapping samples \parencite{jiang2003}. Moreover \textcite{christensen2001} found that under certain measurement errors such as overlapping samples statistical tests are no longer meaningful. Taking this measurement errors into account results in support of the hypothesis, that implied volatility subsumes all information contained in historic volatility. All in all, this insights provide a convincing argument why early studies found the forecast of implied volatility to be inefficient \parencite{jiang2003}. For a very comprehensive overview of the volatility research as was state of the art in 2003, please see \textcite{poon}. \\
%
However, even though the recent evidence is in support of \gls{BS} implied volatility, there are several disadvantages of using the implied volatility with an asset pricing model. One disadvantage is, that the \gls{BS} implied volatility relies mostly on information from at-the-money options, which are generally the most actively traded ones. This however fails to incorporate information contained in other options. Moreover and more importantly, testing for the \gls{BS} implied volatility always means testing market efficiency and the \gls{BS} model jointly. Assumptions of the \gls{BS} model include constant volatility, no transaction costs or taxes, no dividend before option maturity, no arbitrage, continuous trading, constant risk-free interest rate, divisible securities and no short sell \parencite{poon2003}. As it is not possible in this setting to test for market efficiency or the pricing assumptions separately, this tests are subject to model misspecification errors \parencite{jiang2003}. Moreover most early paper's implied volatility ignores the early exercise opportunity and/or dividends \parencite{blair2001}. This is why an implied volatility that does not rely on an asset pricing model was introduced. 


\subsubsection{Model-Free Implied Volatility and the VIX}\label{sec:223VIX}
The idea of model free implied volatility was introduced by \textcite{britten2000} who show that the risk-neutral realized volatiltiy can be derived from a set of options with matching expiration, thus extendending the approach of \textcite{derman1994} \textcite{dupire1994}, \textcite{dupire1997} and \textcite{rubinstein1994} on implied distributions \parencite{jiang2003}. The idea of model free implied volatility is, that insted of specifing a precess for the price of the underlying security and then to derive the option price as a function of this prices process parameters,  a complete set of option prices is taken as given and then as much information as possible is extracted out of the underyling price process \parencite{britten2000}. To be more specific, \textcite{britten2000} show in an approach resembling a binominal tree, that the probability of the stock price reaching any particular level and of a price move is determined by the initial set of option prices. This leads them to their proposition 1, that the expectation of the squared return, conditional on stock price and time, is determined by the initial option prices. Moreover, as this proposition one only infers a one-period forecast conditional on a stock price level, they propose a forecast over any multiperiod interval without conditioning, by showing that the risk-neutral expected sum of squared returns between to dates is given from the set of options expiring on these to dates (their proposition 2). Thiis formula does not use any specific option pricing model to derive implied volatility and is only based on no-arbitrage conditions, therefore it solves the joint-hypothesis problem and test directly for market efficiency \parencite{jiang2003}.  \\
Several papers used this result, examined whether the model free implied volatility subsumes the information from the historic and \gls{BS} implied volatility, conducted forecasts using the model free implied volatility and extended the concept.\\
(write down papers that work with model free implied volatility, e.g. \citeauthor{jiang2003} extended the model so that is not derived under diffusion assumptions and generalized it to processes including random jumps. )\\
One of the first implemented implied volatility index was the VIX from the \gls{CBOE}. It is computed each trading day on a real-time basis, and to facilitate comparison it was calculated back to January 1986. It's introduction in 1993 had the intention to provide a benchmark for market- volatility in the short term, and a volatility index on which futures and options could be written and traded. At this time the VIX was based on the S\&P 100 options, as at this time the S\&P 100 options where the most actively traded in the U.S., which is critical to the usefullness of the VIX or any other implied volatiltiy index. During the years the index option market changed and in 2003 the VIX adopted to this change. First, the S\&p 500 option market superseded the S\&P 100 options market as the most actively traded option market in the U.S., thus following 2003 the VIX was based on the S\&P 500 options. Secondly, option index trading behavior changed. Whereas in the 1990s both call and put index options where equally important, over the years out-of-the money and at-the-money put gained popularity as they were bought by portfolio insurers. Thus the VIX also started to include out-of-the money options in it's calculation, bringing another advantage that was not included in other early implied volatiltiy indexes as mentioned above, which lacked this information conten \parencite{whaley2008}. Finally, the VIX calculation changed, adopting to the model-free implied volatility approach, which was in 2003 widely used by financial theorists, risk managers, and volatility traders alike \parencite{Cboe2009}.\\
\textcolor{gray} {Sometimes the VIX is called \emph{investor fear gauge}, however it is important to notice that it measures, not causes market volatility. It is true, that using regression analysis \textcite{whaley2008} found, that the rate of change in the VIX and S\&P 500 is asymmetric, with the VIX reacting higher to a drop in the S\&P 500 than its rise, which could be interpreted as a higher fear in the downside then excitement in an up-move. Nevertheless, this correlation mus not express causality \parencite{whaley2008}.} The VIX is constructed in the way that it eliminates mis-specification and ``smile'' effects thus making it an accurate measurement of implied volatility \parencite{blair2001}. The formula for calculation is
\begin{align}\label{eq: VIX}
VIX = \sqrt{\frac{2}{T} \sum_{i} \frac{\Delta K_{i}}{K_{i}^{2}} e^{RT} Q(K_{i}) - \frac{1}{T} (\frac{F}{K_{0}} - 1)^{2}} \times 100
\end{align}
with $T$ being time to expiration , $F$ the forward index level, $K_{0}$ the first strike below the forward index level, $K_{i}$ the strike of the $i^{th}$ out-of-the-money option, $\Delta K_{i}$ the interval between the strike prices, $R$ the risk-free rate and $Q(K_{i})$ the midpoint of bid-ask spread for each option with strike $K_{i}$. For the calculation first the options used for calculation are selected. The VIX always uses near- and next-term put and call options, with more than 23 and less than 37 days to expiration. The VIX will always reflect an interpolation of the volatility between these two option maturities, using the midpoint bid-ask price as the transaction price are subject to bid-ask bounde \parencite{poon2003}. Once a week the options to calculate the VIX roll over to new maturities. Then the volatility for both the near- and next-term options is calculated. Out of this two volatility, the 30-day-weighted average is calculated, taken as square root and multiplied by 100 to get the VIX \parencite{exchange2009}. 


%(\textcite{canina1993} for example use only a 4-year range of data from 1983 to 1987)
