\begin{itemize}\itemsep0pt
\item key input to risk measures (VaR etc)
\item key input to pricing derivative securities
\item financial market stability
\item Problems: can not be directly observed and thus has to be estimated  
\end{itemize}

THE JOINT DISTRIBUTIONAL characteristics of asset returns are pivotal for many
issues in financial economics. They are the \textbf{key ingredients for the pricing of
financial instruments}, and they speak directly to the risk-return tradeoff central
to portfolio allocation, performance evaluation, and managerial decision-making. 
fractiles of conditional portfolio
return distributions, which govern the likelihood of extreme shifts in portfolio
value and are therefore central to financial risk management, figuring promi-nently in both regulatory and private-sector initiatives. The most critical feature of the conditional return distribution is arguably itssecond moment structure, which is empirically the dominant time-varying char-acteristic of the distribution. \parencite{andersen2018}.


\textcolor{gray}{
Volatility is for example both a key input factor to risk measures (such as Value-at-risk), or pricing of derivative securities, which both again are crucial for financial decision making. As volatility can not be observed as directly as price can, it has to be both estimated and forecasted. There are multiple ways to forecast volatility. The strenght of a volatility model however lies in it's out-of-sample forecasting power \parencite{poon2003}. \\
Volatility measures play an important role for financial market stability.\\
Stylized facts of financial market data suggests that return distributions are not i.i.d., meaning that the variance of returns over a long horizon can not be derived from a single observed period \parencite{poon2003}. }


A comfortable feature of the continuously compounded returns, is that they are time additive, meaning
\begin{align*}
r(t,h) = r(t) - r(t-h) \ with \ 0 \leq h \leq t \leq T.
\end{align*}
Assuming, that the asset prices are positive and finite, both price and return are defined in the interval [0,T] and as a consequence, r(t) has only countable many jump points in [0,T]. \\
Assuming furthermore that the return process is a càdlàg process, that there are no arbitrage opportunities and frictions and  that the expected return is finite, then the log-price process must constitute a semi-martingale. This leads to the following decomposition of the instantaneous return, into an expected return component and a martingale innovation
\begin{align*}
r(t) = p(t) - p(0) = \mu(t) + M(t) = \mu(t) + M^{c}(t) + M^{j}(t)
\end{align*}
where $\mu(t)$ is a predictable and finite variation process, $M(t)$ is a local martingale which may be further decomposed into $M^{c}(t)$, a continuous sample path, infinite variation local martingale component, and $M^{j}(t)$, a compensated jump martingale.\\
Unfortunately, instantaneous returns can not be observed, and even in liquid markets microstructure effects distort the observation of an even closely continuous sample-path realization. Consequently this decomposition has to be transferred to the discrete interval setting. This is slightly complex, and for this work shall only be constituted, that that in discrete time there are two distinct terms in the return innovation instead of one, however one of these terms is a martingale component, too, and this is the dominant part. 

\begin{figure}[!htbp]
\caption{Level-level regression}
\centering
\input{tables/regression_harvix.tex}
\end{figure}
%
\begin{figure}[!htbp]
\caption{log-log regression}
\centering
\input{tables/regression_harvixln.tex}
\end{figure}
%
\input{tables/AICandBIC.tex}

% Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Fr, Jan 11, 2019 - 17:06:49
\begin{table}[!htbp] \centering 
  \caption{summary statistics} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Skewness} & \multicolumn{1}{c}{Kurtosis} \\ 
\hline \\[-1.8ex] 
RealizedVolatility & 4,481 & 0.110 & 8.802 & 0.877 & 0.613 & 3.110 & 17.685\\ 
VIX.Close & 4,510 & 0.576 & 5.094 & 1.198 & 0.522 & 2.552 & 9.877\\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 


The fact that volatility is not directly observable evolves from the fact that one can only observe one realization of the underlying data generating process, and even though stocks are traded and thus have market prices which could be used for volatility measurement, there is no continuous data available and even for high-frequency data and extremely liquid markets microstructure effects and noise prevent getting close to a continuous sample path. It is thus only possible to estimate avearages of discrete volatility for a given period of time. \parencite{andersen2001}.\\

There are however several approaches that should be introduced here. To start with, the definition of the simple gross return is
\begin{align}\label{eq:return}
1+ R_{t} = \frac{P_{t}}{P_{t-1}} 
\end{align}
In the continuous-time setting, continuously compounded returns are used, which are given by
\begin{align}\label{eq:log-return}
r_{t} = ln(1 + R_{t}) = ln (\frac{P_{t}}{P_{t}} + \frac{P_{t-1} - P_{t}}{P_{t}}) = 
ln \frac{P_{t}}{P_{t-1}} = p_{t} - p_{t-1} \ 
with\  p_{t} = ln(P_{t})
\end{align}

%Volatility is the most dominant time-varying distribution characteristic \parencite{andersen2003}.
%Volatility ``seeks to capture the strength of the (unexpected) return variation over a given period of time'' \parencite[p.7]{andersen2001}


 Financial data also show evidence of scaling as described firstly by Mandelbrot, which is connected to the idea that patterns appear in different times, or that the distribution for returns has similar functional forms for various choices of the time interval. \\
 
 \subsection{The Return Process and Stylized Facts of Financial Data}\label{sec:21ReturnProcess}
 Moreover \citeauthor{andersen2001} mentions that first, even though raw returns have a leptocurtic distribution, the returns standardized by realized volatility are approximately Gaussian. Second, the distribution of realized volatility of returns itself is right skewed, the one of the logarithms of realized volatility however are also approximately Gaussian. Third, the long-run dynamics of realized logarithmic volatilities are well approximated by a fractionally-integrated long-memory process. Other authors who mention stylized facts: \parencite{jiang2003}.

Moreover, even though volatility is not directly observable, it has some properties that are commonly seen. Here the characteristics as presented by \citeauthor{tsay2005} should be introduced. These are firstly volatility clusters, loosely speaking meaning that high volatility tends to be followed by high volatility, and the same is true for low volatility periods. Moreover, volatility jumps are rare. Third, volatility varies within a fixed range and does not diverge to infinity, which statistically means that volatility is often (weakly) stationary. Fourth, there is a leverage effect in volatility, meaning that it reads differently to a big increase in price than to a drop. 

The aim of the HAR-RV model introduced later is to model volatility ex-post and then maybe use this approach to measure the information content of implied volatility and maybe also to build a forecast model. Thus the model needs some measure of latent volatility process, so that the performance can be evaluated. \\

Realized volatility, defined as the square root of the sum of squared high frequency returns can be used to approximate the quadratic variation process which is the variation in a continuous time setting.

\citeauthor{andersen2001} show not only that the realized variance is an \emph{unbiased} estimator of ex-ante expected variance, or at least approximately unbiased when relaxing the zero mean assumption and taking a high sample frequency (their proposition 4). Moreover, \citeauthor{andersen2001} show that the realized variance is a \emph{consistent} nonparametric measure of the notional variance for increasingly finely sampled returns over any fixed length interval (their proposition 5).\\


Having introduced the concept of notional volatility and it's approximation by realized volatility, we now turn to volatility modelling/measurement. To measure volatility, one can separate between parametric and non-parametric methods. Whereas parametric methods try to measure the expected volatility making different assumptions about both the functional form and the variables in the information set available, non-parametric methods try to quantify notional volatility directly. The realized volatility is an example for a non-parametric methods. However, to estimate volatility ex-ante (and to potentially forecast later), this paper will refer to one type of the parametric methods based on the paper of \textcite{corsi2009} termed the HAR-RV model.\\

In the last paragraph, volatility was measured as the average over discrete time. For the HAR-RV model this concept needs to be transferred to an instantaneous volatility measurement in continuous time. Moving to continuous time, the corresponding logarithmic price process is then given by \textcite{corsi2009} as the following stochastic difference equation,

 For the volatility the difference is, that now the notional variance equals the \emph{integrated variance (IV)}, which is the integral of the stochastic process. For a period of one day, this is represented by

\textcite{corsi2009} then take the square-root of this integrated variance and term it \emph{integrated volatility}. \textcite{andersen2001} show that also in this continuous-time setting the integrated variance equals the notional variance, and thus can also be approximated with the sum of squared returns. This leads to the following notion of realized volatility over the one day interval

\begin{align}\label{eq:ex-post-latent-vola}
\sigma_{t+1d}^{(d)} = RV_{t+1d}^{(d)} + w_{1+1d}^{(d)}.
\end{align}
where the $w_{t}^{(d)}$ now subsumes both the latent volatility measurement and estimation errors.  Taking equations \ref{eq:cascade-model} and \ref{eq:ex-post-latent-vola} together, we obtain the time series representation from the cascade model,

They use a longer time series, take into account the 1987 regime shift and use non-overlapping samples \parencite{jiang2003}.

\textcite{canina1993} for example found that implied volatility does not incorporate the information from historic volatility and constitute poor forecast of future volatility using (\gls{SPX}) options.

\textcite{day1992} examine information content from S\&P 100 options between 1983 and 1989 and find that they have significant information content for realized volatility.

For example \textcite{christensen1998} find that implied volatility subsumes the information content of past volatility and outperforms it in forecasting future volatility. 

Moreover most early paper's implied volatility ignores the early exercise opportunity and/or dividends \parencite{blair2001}

It is however important to note, that all of these procedures depend on the assumptions that are made in the asset pricing model \parencite{andersen2001}.

\subsection{Review of Empirical Results on Measuring the Information Content of Model-Free Implied Volatility}\label{sec:31LiteratureResults}

% PAPERS TESTING THE VIX WITH THE OLD INFORMATION CONTENT%
Firstly, there are numerious papers testing the informational efficiency of the VIX before 2003, for example \textcite{blair2001}. These papers are however not included in this literature review section, as they do not use the currently applied model-free implied volatility version of the VIX, which was only introduced in 2003. \\
% PAPERS TESTING MODEL-FREE IMPLIED VOLA %

% PAPERS EXTENDING VIX OR USING IT OTHERWISE %
Apart from the papers examining informational efficeiency, there are  papers using and extending the implied-volatility approach are for ecample \textcite{hao2013}, who extend the VIX by deriving the VIX formulas under a risk-neutral valuation relationship, finding that this GARCH implied volatility is significantly lower than the VIX thus interpreting that the GARCH model can not capture the variance premium. \\

%\usepackage{glossaries} % abbreviations
%\setacronymstyle{long-short}  
%\newacronym{BS}{BS}{Black-and-Scholes-Merton Model}
%\newacronym{CBOE}{CBOE}{Chicago Board of Options Exchange}
%\newacronym{SPX}{SPX}{S\&P 500}
%\makeglossaries

\subsubsection{IV Regression}
In the following, the results of the IV Regression are displayed.

\input{tables/test1.tex}
\input{tables/test2.tex}

% So if you looked away to round your eyes, dear
And I'm building you up
The illusion of love
Touch you look obliged
To a stomp mirror
These visions of trust
The ELISION of love
Tried my best to make you understand me
Do you lips like even
When I turn away?
Try to find a heart where both your hands meet
'Cause I'm falling deeper
With each cross I make
For you I'll trip away
And your body won't ever fade
'Cause you are my

%\input{tables/newey3.tex}\label{tab:newey3}
%
%\input{tables/newey4.tex}\label{tab:newey3}

%\input{tables/overlap3.tex}\label{tab:overlap3}

%\input{tables/overlap4.tex}\label{tab:overlap4}

%\input{./tex/3_Literature-Review.tex}
